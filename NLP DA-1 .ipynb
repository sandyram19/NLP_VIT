{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa4fca0d",
   "metadata": {},
   "source": [
    "<h1><center><strong>CSE4022 Natural Language Processing<br>\n",
    "Digital Assignment -1</strong></center><br>\n",
    "20BCE1091<br>Santhosh Ram G K<h1>\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5625aea4",
   "metadata": {},
   "source": [
    "<h3><strong>1. Utilize Python NLTK (Natural Language Tool Kit) Platform and do the following. \n",
    "Install relevant Packages and Libraries</strong><h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25433a52",
   "metadata": {},
   "source": [
    "<h6>• Explore Brown Corpus and find the size, tokens, categories <br>\n",
    "• Find the size of word tokens</h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e95dfca",
   "metadata": {},
   "source": [
    "\n",
    "The Brown Corpus is a widely-used corpus of text in natural language processing, consisting of texts from a variety of sources, including press reports, fiction, and academic texts.\n",
    "\n",
    "In NLTK, you can access the Brown Corpus using the nltk.corpus module. To see the size of the corpus, you can use the len() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eff33acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to C:\\Users\\SANTHOSH\n",
      "[nltk_data]     RAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1161192\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "print(len(nltk.corpus.brown.words()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768df072",
   "metadata": {},
   "source": [
    "This will give you the total number of words in the corpus.\n",
    "\n",
    "<strong>• Find the size of word types</strong> <br><br>\n",
    "To see the number of tokens (unique words) in the corpus, you can use the set() function to create a set of unique words, and then use the len() function to get the size of the set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9674538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56057\n"
     ]
    }
   ],
   "source": [
    "print(len(set(nltk.corpus.brown.words())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ac701b",
   "metadata": {},
   "source": [
    "To see the categories of texts included in the corpus, you can use the categories() function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75681043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.corpus.brown.categories())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca967dc",
   "metadata": {},
   "source": [
    "<br><strong>•Find the size of the category “government”</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b77bd7",
   "metadata": {},
   "source": [
    "To find the size of the \"government\" category in the Brown Corpus, you can use the words() function from nltk.corpus.brown and pass in \"government\" as the category argument.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2989fa70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70117\n"
     ]
    }
   ],
   "source": [
    "government_words = nltk.corpus.brown.words(categories=[\"government\"])\n",
    "print(len(government_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9ae559",
   "metadata": {},
   "source": [
    "Alternatively, you can use the sents() function to get the sentences of the specific category \n",
    "and use the len() function to get the number of sentences in the category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a4c6f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3032\n"
     ]
    }
   ],
   "source": [
    "government_sents = nltk.corpus.brown.sents(categories=[\"government\"])\n",
    "print(len(government_sents))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad15850",
   "metadata": {},
   "source": [
    "<br><strong>• Count the number of sentences<strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f078afc5",
   "metadata": {},
   "source": [
    "To list the most frequent tokens (words) in the Brown Corpus, you can use the nltk.FreqDist() class and pass in the list of tokens from the corpus. This will create a frequency distribution object that can be used to find the most common words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd38a2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 62713), (',', 58334), ('.', 49346), ('of', 36080), ('and', 27915), ('to', 25732), ('a', 21881), ('in', 19536), ('that', 10237), ('is', 10011)]\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.corpus.brown.words()\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "print(fdist.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baae718b",
   "metadata": {},
   "source": [
    "<br><h3>2. Explore the corpora available in NLTK (any two)  </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37972be",
   "metadata": {},
   "source": [
    "<h5>Raw corpus</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a725bd9",
   "metadata": {},
   "source": [
    "1)The gutenberg corpus: <br>This corpus contains 25,000 free electronic books, including many classics of literature. The texts are in various languages and have been cleaned and formatted for use in NLP. You can access the raw text of the corpus using the nltk.corpus.gutenberg.raw() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a4740c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to C:\\Users\\SANTHOSH\n",
      "[nltk_data]     RAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\gutenberg.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "\n",
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
      "and happy disposition, seemed to unite some of the best blessings\n",
      "of existence; and had lived nearly twenty-one years in the world\n",
      "with very little to distress or vex her.\n",
      "\n",
      "She was the youngest of the two daughters of a most affectionate,\n",
      "indulgent father; and had, in consequence of her sister's marriage,\n",
      "been mistress of his house from a very early period.  Her mother\n",
      "had died too long ago for her to have more than an indistinct\n",
      "remembrance of her caresses; and her place had been supplied\n",
      "by an excellent woman as governess, who had fallen little short\n",
      "of a mother in affection.\n",
      "\n",
      "Sixteen years had Miss Taylor been in Mr. Woodhouse's family,\n",
      "less as a governess than a friend, very fond of both daughters,\n",
      "but particularly of Emma.  Between _them_ it was more the intimacy\n",
      "of sisters.  Even before Miss Taylor had ceased to hold the nominal\n",
      "office of governess, the mildness o\n"
     ]
    }
   ],
   "source": [
    "nltk.download('gutenberg')\n",
    "gutenberg_raw = nltk.corpus.gutenberg.raw()\n",
    "print(gutenberg_raw[:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd02ef39",
   "metadata": {},
   "source": [
    "2) The webtext corpus: <br>This corpus contains a collection of web pages that were collected in the late 1990s and early 2000s. The texts are from different web sites and are in English. You can access the raw text of the corpus using the nltk.corpus.webtext.raw() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae8b4df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package webtext to C:\\Users\\SANTHOSH\n",
      "[nltk_data]     RAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\webtext.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cookie Manager: \"Don't allow sites that set removed cookies to set future cookies\" should stay checked\r\n",
      "When in full screen mode\r\n",
      "Pressing Ctrl-N should open a new browser when only download dialog is left open\r\n",
      "add icons to context menu\r\n",
      "So called \"tab bar\" should be made a proper toolbar or given the ability collapse / expand.\r\n",
      "[XUL] Implement Cocoa-style toolbar customization.\r\n",
      "#ifdefs for MOZ_PHOENIX\r\n",
      "customize dialog's toolbar has small icons when small icons is not checked\r\n",
      "nightly builds and tinderboxen for Phoenix\r\n",
      "finish tearing prefs UI to pieces and then make it not suck\r\n",
      "\"mozbrowser\" script doesn't start correct binary\r\n",
      "Need bookmark groups icon\r\n",
      "Dropping at top of palette box horks things\r\n",
      "keyboard shortcut for Increase Text Size is broken\r\n",
      "default phoenix bookmarks\r\n",
      "[cust] need a toolbar spacer and spring spacer for customize\r\n",
      "Can't launch phoenix while Mozilla is running (or vice versa)\r\n",
      "separator not available when all toolbar items are in toolbar layout\r\n",
      "history menu f\n"
     ]
    }
   ],
   "source": [
    "nltk.download('webtext')\n",
    "webtext_raw = nltk.corpus.webtext.raw()\n",
    "print(webtext_raw[:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f5cdad",
   "metadata": {},
   "source": [
    "<br><h4>POS Tagged</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1361874",
   "metadata": {},
   "source": [
    "1) The treebank corpus:<br>The Penn Treebank is a POS-tagged corpus of text from the Wall Street Journal. The corpus includes over 4.5 million words, and includes POS tags for each word. POS-tagged version of the Treebank Corpus can be accessed using the nltk.corpus.treebank.tagged_words() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f50aec7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to C:\\Users\\SANTHOSH\n",
      "[nltk_data]     RAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\treebank.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.'), ('Mr.', 'NNP'), ('Vinken', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "nltk.download('treebank')\n",
    "treebank_tagged_words = nltk.corpus.treebank.tagged_words()\n",
    "print(treebank_tagged_words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aea4055",
   "metadata": {},
   "source": [
    "2)The nps_chat corpus:<br> The NPS Chat Corpus is a POS-tagged corpus of instant messaging text. The corpus includes over 10,000 posts and instant messages, and includes POS tags for each word. POS-tagged version of the nps_chat Corpus can be accessed using the nltk.corpus.nps_chat.tagged_words() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b395cbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('now', 'RB'), ('im', 'PRP'), ('left', 'VBD'), ('with', 'IN'), ('this', 'DT'), ('gay', 'JJ'), ('name', 'NN'), (':P', 'UH'), ('PART', 'VB'), ('hey', 'UH'), ('everyone', 'NN'), ('ah', 'UH'), ('well', 'UH'), ('NICK', 'NN'), (':', ':'), ('U7', 'NNP'), ('U7', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('gay', 'JJ')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package nps_chat to C:\\Users\\SANTHOSH\n",
      "[nltk_data]     RAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package nps_chat is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('nps_chat')\n",
    "nps_chat_tagged_words = nltk.corpus.nps_chat.tagged_words()\n",
    "print(nps_chat_tagged_words[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878b5462",
   "metadata": {},
   "source": [
    "<h3>3. Create a text corpus with a minimum of 200 words (unique content).<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "373328a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\SANTHOSH\n",
      "[nltk_data]     RAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\SANTHOSH\n",
      "[nltk_data]     RAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\SANTHOSH RAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\SANTHOSH\n",
      "[nltk_data]     RAM\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fd5500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data. It involves the collection, cleaning, integration, analysis, and interpretation of data, and the communication of findings and insights to stakeholders. Data science is a rapidly growing field, driven by the explosion of data in recent years, and the increasing need to extract insights and make data-driven decisions in a wide range of domains, including business, healthcare, finance, and science.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0e1cbe",
   "metadata": {},
   "source": [
    "Implement the \n",
    "following text processing \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ab03e1",
   "metadata": {},
   "source": [
    "<h5>Word Segmentation</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166803d3",
   "metadata": {},
   "source": [
    "Word segmentation, also known as word tokenization, is the process of breaking down a text into individual words. In natural language processing (NLP), word segmentation is an important step in preparing text data for further analysis. It is the process of splitting a text into words, phrases, symbols, or other meaningful elements, known as tokens.\n",
    "\n",
    "In simple terms, it's the process of breaking down a sentence into words. It separates the text into words and punctuation marks, so that it can be processed more easily. This is important because words have different meanings and grammatical roles, and so it is important to be able to identify them separately.\n",
    "\n",
    "For example, the sentence \"The quick brown fox jumps over the lazy dog.\" would be segmented into the following tokens: \"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"\n",
    "\n",
    "NLTK provides several word tokenization methods, such as word_tokenize() and sent_tokenize() functions which can be used for word segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82798714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word segmentation: ['Data', 'science', 'is', 'an', 'interdisciplinary', 'field', 'that', 'uses', 'scientific', 'methods', ',', 'processes', ',', 'algorithms', ',', 'and', 'systems', 'to', 'extract', 'knowledge', 'and', 'insights', 'from', 'structured', 'and', 'unstructured', 'data', '.', 'It', 'involves', 'the', 'collection', ',', 'cleaning', ',', 'integration', ',', 'analysis', ',', 'and', 'interpretation', 'of', 'data', ',', 'and', 'the', 'communication', 'of', 'findings', 'and', 'insights', 'to', 'stakeholders', '.', 'Data', 'science', 'is', 'a', 'rapidly', 'growing', 'field', ',', 'driven', 'by', 'the', 'explosion', 'of', 'data', 'in', 'recent', 'years', ',', 'and', 'the', 'increasing', 'need', 'to', 'extract', 'insights', 'and', 'make', 'data-driven', 'decisions', 'in', 'a', 'wide', 'range', 'of', 'domains', ',', 'including', 'business', ',', 'healthcare', ',', 'finance', ',', 'and', 'science', '.']\n"
     ]
    }
   ],
   "source": [
    "# Word segmentation\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(\"Word segmentation:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa1e8db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence segmentation: ['\\nData science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data.', 'It involves the collection, cleaning, integration, analysis, and interpretation of data, and the communication of findings and insights to stakeholders.', 'Data science is a rapidly growing field, driven by the explosion of data in recent years, and the increasing need to extract insights and make data-driven decisions in a wide range of domains, including business, healthcare, finance, and science.']\n"
     ]
    }
   ],
   "source": [
    "# Sentence segmentation\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "print(\"Sentence segmentation:\", sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "adfb2b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'science', 'is', 'an', 'interdisciplinary', 'field', 'that', 'uses', 'scientific', 'methods', ',', 'processes', ',', 'algorithms', ',', 'and', 'systems', 'to', 'extract', 'knowledge', 'and', 'insights', 'from', 'structured', 'and', 'unstructured', 'data', '.', 'it', 'involves', 'the', 'collection', ',', 'cleaning', ',', 'integration', ',', 'analysis', ',', 'and', 'interpretation', 'of', 'data', ',', 'and', 'the', 'communication', 'of', 'findings', 'and', 'insights', 'to', 'stakeholders', '.', 'data', 'science', 'is', 'a', 'rapidly', 'growing', 'field', ',', 'driven', 'by', 'the', 'explosion', 'of', 'data', 'in', 'recent', 'years', ',', 'and', 'the', 'increasing', 'need', 'to', 'extract', 'insights', 'and', 'make', 'data-driven', 'decisions', 'in', 'a', 'wide', 'range', 'of', 'domains', ',', 'including', 'business', ',', 'healthcare', ',', 'finance', ',', 'and', 'science', '.']\n"
     ]
    }
   ],
   "source": [
    "# Convert to lowercase\n",
    "tokens = [token.lower() for token in tokens]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ce8499a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#Stop words removal\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8cc1bc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words removal: ['data', 'scienc', 'interdisciplinari', 'field', 'use', 'scientif', 'method', ',', 'process', ',', 'algorithm', ',', 'system', 'extract', 'knowledg', 'insight', 'structur', 'unstructur', 'data', '.', 'involv', 'collect', ',', 'clean', ',', 'integr', ',', 'analysi', ',', 'interpret', 'data', ',', 'commun', 'find', 'insight', 'stakehold', '.', 'data', 'scienc', 'rapidli', 'grow', 'field', ',', 'driven', 'explos', 'data', 'recent', 'year', ',', 'increas', 'need', 'extract', 'insight', 'make', 'data-driven', 'decis', 'wide', 'rang', 'domain', ',', 'includ', 'busi', ',', 'healthcar', ',', 'financ', ',', 'scienc', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = [token for token in tokens if token not in stop_words]\n",
    "print(\"Stop words removal:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a684cd",
   "metadata": {},
   "source": [
    "<h5>Stemming</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d438400c",
   "metadata": {},
   "source": [
    "Stemming is the process of reducing words to their base or root form. The goal of stemming is to reduce words to their base form so that words that are derived from the same root word can be identified and analyzed as the same word.\n",
    "\n",
    "For example, the words \"jumping,\" \"jumps,\" and \"jumped\" would all be reduced to the root form \"jump.\"\n",
    "\n",
    "Stemming is commonly used in natural language processing (NLP) tasks such as text classification, information retrieval, and text mining, to help reduce the dimensionality of the text data and increase the computational efficiency of the text analysis.\n",
    "\n",
    "There are several stemmer algorithms available, such as Porter stemmer, Snowball stemmer, and Lancaster stemmer. NLTK library provides several stemmers like PorterStemmer, SnowballStemmer and LancasterStemmer.\n",
    "\n",
    "Porter stemmer algorithm is the most widely used stemmer algorithm. It uses a set of heuristic rules to iteratively remove the common morphological and inflectional affixes from words, and it's widely used in the english language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2633b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming: ['data', 'scienc', 'interdisciplinari', 'field', 'use', 'scientif', 'method', ',', 'process', ',', 'algorithm', ',', 'system', 'extract', 'knowledg', 'insight', 'structur', 'unstructur', 'data', '.', 'involv', 'collect', ',', 'clean', ',', 'integr', ',', 'analysi', ',', 'interpret', 'data', ',', 'commun', 'find', 'insight', 'stakehold', '.', 'data', 'scienc', 'rapidli', 'grow', 'field', ',', 'driven', 'explos', 'data', 'recent', 'year', ',', 'increas', 'need', 'extract', 'insight', 'make', 'data-driven', 'decis', 'wide', 'rang', 'domain', ',', 'includ', 'busi', ',', 'healthcar', ',', 'financ', ',', 'scienc', '.']\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "tokens = [stemmer.stem(token) for token in tokens]\n",
    "print(\"Stemming:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18257e4a",
   "metadata": {},
   "source": [
    "<h5>Lemmatization</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fe14b0",
   "metadata": {},
   "source": [
    "Lemmatization is a process of reducing a word to its base or root form. The root form of a word is called its lemma. Lemmatization is similar to stemming, which is the process of reducing words to their base form, but it is more accurate and uses a vocabulary and morphological analysis of words to get the root form.\n",
    "\n",
    "Lemmatization takes into consideration the context of the word, its part of speech and morphological form, as well as its meaning and uses a dictionary-based approach to obtain the correct lemma of a word. This makes it more accurate than stemming, which uses heuristic rules to remove common morphological and inflectional affixes.\n",
    "\n",
    "For example, the words \"running,\" \"ran\" and \"run\" would all be reduced to the root form \"run\" by lemmatization.\n",
    "\n",
    "stemming is a simpler and faster process, but it can lead to inaccuracies and non-existing words, while lemmatization is a more accurate process but it requires more computational resources and it's more complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "601fd7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\SANTHOSH\n",
      "[nltk_data]     RAM\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40ae7efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization: ['data', 'scienc', 'interdisciplinari', 'field', 'use', 'scientif', 'method', ',', 'process', ',', 'algorithm', ',', 'system', 'extract', 'knowledg', 'insight', 'structur', 'unstructur', 'data', '.', 'involv', 'collect', ',', 'clean', ',', 'integr', ',', 'analysi', ',', 'interpret', 'data', ',', 'commun', 'find', 'insight', 'stakehold', '.', 'data', 'scienc', 'rapidli', 'grow', 'field', ',', 'driven', 'explos', 'data', 'recent', 'year', ',', 'increas', 'need', 'extract', 'insight', 'make', 'data-driven', 'decis', 'wide', 'rang', 'domain', ',', 'includ', 'busi', ',', 'healthcar', ',', 'financ', ',', 'scienc', '.']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "print(\"Lemmatization:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6b1d4b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tagging: [('data', 'NNS'), ('scienc', 'NNS'), ('interdisciplinari', 'VBP'), ('field', 'NN'), ('use', 'NN'), ('scientif', 'JJ'), ('method', 'NN'), (',', ','), ('process', 'NN'), (',', ','), ('algorithm', 'NN'), (',', ','), ('system', 'NN'), ('extract', 'JJ'), ('knowledg', 'NN'), ('insight', 'NN'), ('structur', 'NN'), ('unstructur', 'JJ'), ('data', 'NNS'), ('.', '.'), ('involv', 'NN'), ('collect', 'NN'), (',', ','), ('clean', 'JJ'), (',', ','), ('integr', 'JJ'), (',', ','), ('analysi', 'JJ'), (',', ','), ('interpret', 'JJ'), ('data', 'NNS'), (',', ','), ('commun', 'NN'), ('find', 'VB'), ('insight', 'JJ'), ('stakehold', 'NN'), ('.', '.'), ('data', 'NNS'), ('scienc', 'VBD'), ('rapidli', 'JJ'), ('grow', 'JJ'), ('field', 'NN'), (',', ','), ('driven', 'JJ'), ('explos', 'NN'), ('data', 'NNS'), ('recent', 'JJ'), ('year', 'NN'), (',', ','), ('increas', 'VBP'), ('need', 'VBP'), ('extract', 'JJ'), ('insight', 'JJ'), ('make', 'VB'), ('data-driven', 'JJ'), ('decis', 'NNS'), ('wide', 'JJ'), ('rang', 'NN'), ('domain', 'NN'), (',', ','), ('includ', 'JJ'), ('busi', 'NN'), (',', ','), ('healthcar', 'NN'), (',', ','), ('financ', 'NN'), (',', ','), ('scienc', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# POS tagging\n",
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "print(\"POS tagging:\", tagged_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca839ed",
   "metadata": {},
   "source": [
    "DT: Determiner, JJ: Adjective, NN: Noun, VBZ: 3rd person singular present verb, IN: preposition, . : punctuation. <br>\n",
    "NNS stands for \"noun, plural.\"<br>\n",
    "VBP stands for \"verb, non-3rd person singular present.\"<br>\n",
    "VBD stands for \"verb, past tense.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
